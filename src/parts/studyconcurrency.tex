\chapter{Study of functional programming abstractions for concurrency and asynchronicity}

The architecture of the platform is heavily based on functional programming concepts to handle concurrency and
asynchronicity in an effective and composable way. The following section describes and compares these abstractions.


\section{Monadic Futures}

\subsection{The problems of blocking IO with threads}

To handle concurrency and IO, traditional languages use native threads and blocking IO. A thread is a unit of execution that is has its own stack
on the underlining OS, and concurrency is achieved by switching threads on the machine cores. For example, with the blocking IO model,
a thread that is waiting for IO is preempted by the OS. Traditionally threads has a high resource cost, both fixed cost (the default stack size for a thread is 
1 MB on a 64 bit JVM), high context switching cost and high creation time. In case of Web-oriented application, a new thread is generally spawn for each new client,
and if the Web application needs to call several backend services (that is usually the case in modern Service Oriented Architecture), 
this thread will be blocked, doing nothing but using stack space and causing context switching.
Such a model has been proved to be inefficient for a large number of concurrent clients for Web applications that calls various backend services
and/or perform stream-oriented connections as highlighted by James Roper \footfullcite{bib:asyncio}. This is ever more
important when backend services can occasionally be slow / fail. In case of a blocking IO model, clients' threads that requests
this failed service will wait for this service (before a timeout), causing a very high number of threads in the server. As James Roper
stated, this high number of threads prevents the other requests (calling another non-failed service) to be performed efficiently,
because the server spends a lots of its time doing context switching between blocked threads that are doing nothing. This is ever worst if
you have a maximum number of threads allowed in the server (that is usually the case in cloud platforms): new clients can not connect at all
to your server because there is no thread to allow to them. Non-blocking IO servers are also known as evented servers.
Mark McGranaghan highlights this in his article about Threaded vs Evented Servers \footfullcite{bib:threadevent}. If we define \verb|c| the CPU time
that each request takes and \verb|w| the total time of the request including waiting time calling external services, an evented server
performs way better than a threaded server when the ratio \verb|w/c| is high (so when most time of a request is spent waiting for
external services).

\subsection{The problems of asynchronous non-blocking IO with callbacks}
In order to avoid the problems caused by blocking IO, one can use a non-blocking IO model: when a thread is doing an IO operation, it doesn't wait
until the IO is finished but rather provide a mechanism to notify the caller when the IO is finished. Meanwhile, the thread can be used for other tasks, like
serving other web clients. 

The problem is that this kind of asynchronous non-blocking programming can easily lead to hard code maintenance if no proper abstraction is used. The common way
of many languages to deal with asynchronicity is to provide a callback mechanism (Javascript may be the language that uses them the most). A callback is
way to perform an asynchronous operation by providing a function as a parameter of the function that do the asynchronous operation. The parameter function
will be \textit{call back} when the asynchronous operation is finished. An example of a GET HTTP request to a web service in Javascript is shown in Listing 
\ref{lst:jscb}.

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{javascript}
performHttpGet("http://www.example.com", function(error, response) {
  if (!error) {
    console.log("Response status: " + response.status)
  }
});
\end{minted}
\caption{A callback in Javascript}
\label{lst:jscb}
\end{listing}

Here \verb|function(error, response) {...}| is the user-defined function that is call back when the asynchronous GET request
returned. We see that callbacks are only about side-effects: no value is returned by the \verb|performHttpGet| function.
This causes a serious lack of composability, popularly known as "callback hell". Listing \ref{lst:jspd} shows how to perform several asynchronous operations
sequentially.

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{javascript}
action1(function(error, response1) {
  if (!error) {
    action2(function(error, response2) {
      if (!error) {
        action3(function(error, response3) {
          if (!error) {
            action4(function(error, response4) {
              // do a side-effect with response4       
            });
          }
        });
      }
    });
  }
});
\end{minted}
\caption{The "pyramid of doom" in Javascript}
\label{lst:jspd}
\end{listing}

Such call is called "pyramid of doom" because the code invariably shifts to the right, and the intermediate steps can not be reused to compose
them later with other operations. 

Moreover, doing concurrent operations with the callback model is not easy also. We want here to perform 2 asynchronous operations in parallel,
and do something with the result (composed of the result of the 2 operations). Listing \ref{lst:jsparr} shows how to do such in standard Javascript.

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{javascript}
  var results = [];

  function doSomethingWithResults(results) {
    // final callback
  }

  action1(function(error, response) {
      results.push(response);
      if (results.length == 2) {
        doSomethingWithResults(results)
      }
    }
  });

  action2(function(error, response) {
    results.push(response);
    if (results.length == 2) {
      doSomethingWithResults(results)
    }
  });
\end{minted}
\caption{Performing two asynchronous operations concurrently in Javascript}
\label{lst:jsparr}
\end{listing}

The fact that the callback model is based on a closures that performs side-effect prevents easy composability. What I mean by composabilty
is the fact of defining independently various asynchronous operations, and then compose them (sequentially, in parallel) to obtain a composed result
of these actions. Moreover, error handling must be done manually for each asynchronous operation. Monadic futures are an abstraction coming from
functional programming that solves these problems.


\subsection{Monadic futures for composable asynchronous non-blocking programming}

A Future is a monadic abstraction that stands for a value that may be available in the future. Using Scala's notation, a future is a type that is 
parametrized by the type of the value that will eventually be available. For example, \verb|Future[Int]| is a type that represents an eventual integer.
With futures, asynchronous functions return a \verb|Future[ResponseType]| instead of taking a callback function as a parameter. Listing \ref{lst:futures}
shows simple future creations.

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{scala}
val futureResponse: Future[HttpResponse] = performHttpGet("http://www.example.com")
val futureComputation: Future[Int] = future {
  // do long computation
}
\end{minted}
\caption{Futures in Scala}
\label{lst:futures}
\end{listing}

We see in Listing \ref{lst:futures} that futures can be used for non-blocking IO, but also as an abstraction for concurrency. In the example, the main thread
executing the code does not block on both methods. The "long computation" will be done in another thread as it is encapsulated by a \verb|future {}|.

Behind the scene, futures are multiplexed into a thread pool named a ExecutionContext in Scala. ExecutionContexts can be passed to methods that returned a future.
This allows to decouple the \textit{concurrency semantic} (which tasks should be run concurrently) from the \textit{concurrency implementation} (an ExecutionContext
can for example limit the number of threads it can use, etc.). Twitter's engineer and researcher Marius Eriksen highlights this idea in his 
"Your Server as a Function" paper \ref{bib:serverfunc} where he states that futures are a declarative data-oriented way of doing asynchronous programming.

Moreover, as Marius Eriksen highlights in his article "Future aren't ersatz threads" \ref{bib:futurenotthreads}, Futures "model the real world truthfully": 
a Future[T] can either result in a success with a value of type T, or with an error (Exception), which is inherently the case with IO operations.
\\

The term \textit{monadic} comes from Monads, a key abstraction in typed functional programming coming from the Haskell world. Thoroughly defining what a monad
is out of the scope of this thesis, but in a few words a monad is a type that encapsulates another type in order to perform operations on it. Some operations
are mandatory to define a monad. Listing \ref{lst:monad} define the trait in Scala to define a monad, coming from the book Functional Programming in Scala
\footfullcite{bib:fpscala}. 

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{scala}
trait Monad[F[_]] extends Functor[F] {
  def unit[A](a: => A): F[A]
  def flatMap[A,B](ma: F[A])(f: A => F[B]): F[B]

  def map[A,B](ma: F[A])(f: A => B): F[B] = flatMap(ma)(a => unit(f(a)))
}
\end{minted}
\caption{The Monad trait in Scala}
\label{lst:monad}
\end{listing}

\verb|unit| allows to construct a monad that encapsulate a value of type A (equivalent to \verb|future {}|), \verb|map| allows to apply a function to 
the encapsulated value, and \verb|flatMap| allows to apply a function to the encapsulated value that returns itself a monad.
\\

A Future is a monadic type, meaning that it extends the monad trait and implement the \verb|unit| and \verb|flatMap| methods.
These methods (and many more) allows powerful compositions between different Futures instances. 

For example, \verb|map| allows to transform the result of an asynchronous operation. \verb|flatMap| allows sequential and 
parallel composition. Listing \ref{lst:futcompo} illustrates these composition.

\begin{listing}[h]
\begin{minted}[fontsize=\small, frame=lines, framesep=2mm]{scala}
/*
 * Sequential composition of asynchronous operations returning Integers
 */
val future1: Future[Int] = action1()
val futureResult4: Future[String] = future1
  .flatMap(result1 => action2(result1))
  .flatMap(result2 => action3(result2))
  .flatMap(result3 => action4(result3))
  .map(result4 => "This is result4: " + result4)

// one can also use Scala's monad comprehension that provides 
// syntaxic sugar to monad operation:
val futureResult4: Future[String] = for {
  result1 <- action1()
  result2 <- action2(result1)
  result3 <- action3(result2)
  result4 <- action4(result3)
} yield "This is result4: " + result4

/*
 * Concurrent composition
 */
val future1: Future[Int] = action1()
val future2: Future[Int] = action2()
val future1And2: Future[(Int, Int)] = future1 zip future2

\end{minted}
\caption{Future composition in Scala}
\label{lst:futcompo}
\end{listing}

We see in \ref{lst:futcompo} that we avoid the "pyramid of doom" effect for sequential composition, and that concurrent composition is very simple
compared to callback-based programming. Moreover, monad operations allows automatic error propagation. In the sequential composition example,
if for instance action2 failed, the action3 and action4 will not be executed, and futureResult4 will be a failed future with the Exception
that action2 throwed. 

In summary, Futures are a simple immutable abstraction that allows easy reasoning and composition about asynchronous programming.
However, a Future only model the fact that \textit{one} value will be available in the future. Hence, it is not directly applicable
to model \textit{asynchronous non-blocking streams}.

\section{Iteratees}



\section{Actor model}

\subsection{The problems of synchronization with threads}

TODO: compare to locks...

TODO: problem: no sequentiality of async op
