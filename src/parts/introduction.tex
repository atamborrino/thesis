\chapter{Introduction}

\section{Context and system overview}

Data is now at the center of organizations and is increasingly heterogeneous with an explosion of data sources 
that each exposes data in its own format that can be structured, semi-structured or non-structured.
Another major trend is that data processing needs to be real-time, because business men no longer want to wait a whole day 
to have reports and alerts on their business data. Last but not least, the volume of data that enterprises need to analyze is constantly growing, which is commonly referred as "Big Data".
\\

To meet these requirements, traditional Data Warehouse software start to be out-dated. They often
propose to deal only with structured data in order to store it in a relational database. Moreover, they are often batch-oriented: 
the ETL mechanism (data extraction, transform and load) regularly happens once or twice per day, and there is no mechanism 
for real-time subscriptions on new data events (as highlighted by Jay Kreps, engineer at LinkedIn, 
in his article "The Log: What every software engineer should know about real-time data's unifying abstraction" \footfullcite{bib:linkedinLog}). Furthermore,
Data Integration, Data Storage and Data Reporting are often coupled into a single monolithic architecture.
\\

Thus, a new kind of architecture for Data Integration and Data Processing is needed in order to meet these new requirements: real-time processing of potentially big volumes of unstructured data. This thesis presents an architecture that solves this problem using \textit{decoupled components} that communicate with \textit{immutable events} that represent data changes. Events flow across the platform enabling components to react to data changes in various ways. Real-time should be understood as \textit{soft} real-time in comparison to batch modes that are more common in Big Data frameworks. For example, event propagation across the platform should be measured in milliseconds or seconds, whereas batch jobs are often measured in hours. Moreover, in a real-time platform, the notion of Big Data is more related to the push rate of events than the size of an event itself. Thus, the platform should take care of possible performance problems in order to handle high push rates.

Each event represents the change (creation, update or deletion) made to a data resource at a particular time. Based on the Event-Sourcing principle \footfullcite{bib:eventSourcing}, events are stored in a Journal that is an ordered sequence of events. Then, the stream of events coming from the Journal can be processed by data consumers that can react to the change of data (see Figure \ref{fig:main_archi} for the global architecture). 
An example of data consumer can be one that maintains a pre-computed view on the data that is updated upon each event, or one that pushes 
notifications to another service upon the reception of some kinds of events.
\\

An example use case is when an organization uses different SaaS services for each of its teams. For instance, the sales
team uses a SaaS software to process their sales pipeline, the project management team uses another SaaS software to manage
the production teams, etc... Without a central data backbone, it is not possible to have a global view on the company data.
The platform I present in this thesis can integrate these different SaaS softwares using their REST API, detect what
changes have been made on the data, and emit the corresponding events. As a result, data consumers can use these events
to update dashboards about the company data in real-time, mixing the data coming from different sources. A data consumer can also push a
notification to SaaS service X when it receives an event from SaaS service Y, allowing real-time synchronization between
heterogeneous services.
\\

An advantage of Event Sourcing is that the whole history of the system is stored. Events are immutable changes made to the data
and are always appended to the Journal (never deleted or modified). As a result, the system stores not only the current state
of the data, but also all its previous states. This allows two interesting properties.

First, it is possible to query past states of the data. This can be very useful for various use cases
where one is interested in the data history, for example a financial audit.

Moreover, storing all the data changes greatly improves the fault-tolerance of the system. As events are not deleted, it is always possible 
to come back in the past in the Journal, delete some delete events that were put by mistake, and replay the events after
them to re-build the system in a right state. This is also referred as Human Fault-Tolerance \footfullcite{bib:human-ft}: in a mutable system,
if an user accidentally delete a data entry, it is lost for ever. But in an immutable system, the deletion is just another
event added to the journal. Figure \ref{fig:event-sourcing} illustrates the difference between a mutable system and an immutable event-sourced system.
\\ 

\begin{figure}[h]
  \begin{center}
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/event-sourcing.png}}
    \caption{Immutable datastore and the Event Sourcing principle}
    \label{fig:event-sourcing}
  \end{center}
\end{figure}

This kind of architecture is also known as CQRS \footfullcite{bib:cqrs} (Command Query Responsibility Segregation). The core principle of CQRS is to decouple the write part and the read part
of a system. The write part (Data Integration) only needs to push immutable events to the Journal in an append-only fashion, which
is very efficient because there is no mutation of the data and no read-write contentions as in traditional databases.
The read part is a set of denormalized pre-computed views that are optimized for low read latency (as the views are automatically re-computed
when a new related event comes in).
An obvious downside of such an architecture is that data is eventually consistent: when a data producer has received the acknowledgment
from the Journal, there is no guarantee that data consumers has already processed the event and updated the data view.

This model also allows very easy distribution of the platform because it enables a message-oriented
architecture where each component (data producer, journal, data consumers with data views) only exchanges messages (events) with each other (share-nothing architecture).
\\

The platform is composed of three main parts: 
\begin{itemize}
  \item Data Integration, that must integrate several data sources in order to emit 
events (data changes) to the Journal. 
  \item Journal, an abstraction for a sequence of immutable events. The Journal must expose methods to insert events,
  and expose methods to subscribe to the stream of events.
  \item Stream processing, where one can define a tree of data consumers (stream processors) that can react to
  events, maintain derived pre-computed views on the data, and emit new streams of events.
\end{itemize}

\begin{figure}[h]
  \begin{center}
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/main_archi.png}}
    \caption{Global architecture}
    \label{fig:main_archi}
  \end{center}
\end{figure}

Nonetheless, this kind of evented architecture must be done with a lot of care concerning technical architecture.
The platform needs to do lot of IO in order to push the stream of events from data sources to data consumers, and must
parallelize a lot of operations. Moreover, it must ensure that the stream of events (producers) does not overwhelm the stream
processors (consumers), i.e. if consumers process data slowly, producers must try to slow the push rate. The platform should also deal with possible
failures of components and offer strong guarantees in these cases (like no message loss or duplication).

In order to fulfill those requirements, the platform will apply the principles of the Reactive Manifesto \footfullcite{bib:reactiveManifesto} in order to guarantee
that the platform is \textit{scalable, event-driven, resilient and responsive} (the four Reactive Traits). An asynchronous non-blocking approach with a share-nothing architecture will be used to develop the platform in order to optimize resource consumption, decouple components to be able to distribute them easily, take easily advantage of parallelization and handle failures. 
The platform is developed using functional programming in the Scala programming language \footfullcite{bib:scala} in order to leverage functional programming abstractions to better handle asynchronous and stream-oriented code.
\\


\section{Related work}

There exist several Big Data frameworks for real-time stream processing. Among them, Apache Kafka \footfullcite{bib:kafka} and Apache Storm \footfullcite{bib:stormframework} have been thoroughly studied for this thesis.
\\

Apache Kafka is a high-throughput distributed messaging system developed at LinkedIn. It uses a distributed publish-subscribe model where data producers can publish events to topics and data consumers can subscribe to topics. It is durable by persisting events on disk and data consumers can pull events with a guaranteed ordering. However, as it uses a publish-subscribe abstraction, it does not enable the user to clearly define stream processing flows (such as trees or DAGs) where components are both data consumers (receiving events from parent components) and data producers (sending events to their child components).
\\

Apache Storm is a distributed and fault-tolerant real-time computation framework developed at Twitter. It enables the user to define a DAG of stream processors that can receive events from their parent(s) and send derived events to their children. However, messages are not persisted on disk, so there is no durability, which implies that slow processors are forced to keep past events in-memory if we want fast processors to move on to the next events without waiting for slow processors (more details will be given on these types of recurrent stream processing issues in chapter \ref{chapt:streamproc}).
\\

As explained in more details in the thesis, our platform will take some architecture patterns of these two frameworks to achieve an original architecture with a list of properties that none of these frameworks fully provide on their own.

\section{Contributions}

The main contributions of this thesis are:
\begin{itemize}
  \item Definition of the architecture of the Data Integration part and its implementation
  \item Definition of the architecture of the Event Stream Processing part and its implementation as a generic library
  \item Implementation of a business use case application using the generic library for event stream processing, as well as performance tests on this application
\end{itemize}




