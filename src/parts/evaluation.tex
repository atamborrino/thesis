\chapter{Evaluation and Conclusion}

\section{Requirement evaluation}

Concerning functional requirements, the platform meets the needs by allowing to define data pullers (actors) that incrementally pull various data sources and various resource types.
The pulled data is processed sequentially in a simple pipeline that aggregate, clean and validate the data before inserting events in the Journal.

Then, the Stream Processing part allows to define a tree of stream processors. A processor can react to events sent by its parent by producing a substream of events towards its children. Substream are inserted in-place in the stream, meaning that the whole substream should be sent to children before processing the next input event. A processor can do a side-effect with a guaranteed exactly-once side-effect semantic, allowing to the user of the library to safely define non-idempotent side-effects.
\\

Concerning the non-functional requirements, all the parts of the architecture are built with an asynchronous non-blocking architecture according to the Reactive Manifesto 
\footfullcite{bib:reactiveManifesto} to optimize performance and resource use by being \textit{event-driven, scalable, resilient an responsive}, the four Reactive traits of the Reactive Manifesto.

The Data Integration part is built on top of an Actor system to allow easy concurrency and distribution to make the best use of resource (CPU, Threads) thanks to a non-blocking implementation. Future and Iteratees are used to model sequential in-order asynchronous stream processing in a simple, composable and maintainable way. A persistence storage
in MongoDB is used to be sure that pullers are fault-tolerant: there is no event loss or duplication even in case of failure of a puller. The system is easily distributable 
thanks location transparency that is inherent to the actor model.

The Stream Processing part uses a complex adaptive push-pull model with back-pressure to allow decoupled stream processing while optimizing the resource use. A stream processor abstraction has been created on top of Iteratees, Futures and Promises. The tree of processors guarantees in-order sequential asynchronous stream processing with fault-tolerance: the temporary failure of a processor is guaranteed without message loss or duplication by allowing a processor to replay the stream from its parent.
\\


\section{Conclusion}

In this thesis has been presented a Reactive platform for Data Integration and Event Stream Processing. Events and Event-Sourcing are the core concepts of the platform to enable 
real-time propagation of data changes across the system, from external data sources to data aggregation dashboards. Data sources are pull in a parallel and incremental way via their REST API to create a real-time flow of events that are inserted in an event-sourced Journal. Saving not only the current state of the data but also all the changes on the data
that led to this current state allows to be able to query and replay the history of the data, which can be useful for business purposes but also for technical purposes such as fault-tolerance or stream processing with subscribers that have heterogeneous processing speed.
From the events stored in the Journal, a tree of stream processors can be defined to subscribe to the data changes and react in various ways. An example use of a processor is to maintain a specific data aggregation dashboard that gets updated incrementally upon certain types of events. Strong technical guarantees are ensured by the platform as in-order sequential asynchronous processing with exactly-once side-effect semantic even in case of failure. Under the hood, a complex adaptive push-pull model with back-pressure is used to maximize the performance of the system and minimize the amount of resource (CPU, RAM) used.
\\

Concerning the Future Work that can be made on the platform, one problem is that the distributed mode is for now point-to-point oriented, i.e we need to manually configure which component (data puller, processor) is on which machine. To obtain elastic scalability, it would be interesting to use a cluster oriented approach to distribution with a cluster manager layer that automatically put components on machines according to the current resource availability of each machine.


