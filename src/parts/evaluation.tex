\chapter{Evaluation and Conclusion}

\section{Requirements evaluation}

Concerning functional requirements, the platform meets the needs by allowing to define data pullers (actors) that incrementally pull various data sources and various resource types.
The pulled data is processed sequentially in a simple pipeline that aggregates, cleans and validates the data before inserting events in the Journal.

Then, the Stream Processing part allows to define a tree of stream processors. A processor can react to events sent by its parent by producing a substream of events towards its children. Substream are inserted in-place in the stream, meaning that the whole substream should be sent to children before processing the next input event. A processor can do a side-effect with a guaranteed exactly-once side-effect semantic, allowing to the user of the library to safely define non-idempotent side-effects.
\\

Concerning the non-functional requirements, all the parts of the architecture are built with an asynchronous non-blocking architecture according to the Reactive Manifesto 
\footfullcite{bib:reactiveManifesto} to optimize performance and resource use by being \textit{event-driven, scalable, resilient an responsive}, the four Reactive Traits.

The Data Integration part is built on top of an Actor system to allow easy concurrency and distribution. Moreover, it makes the best use of resources (CPU, Threads) thanks to a non-blocking implementation. Futures and Iteratees are used to model sequential in-order asynchronous stream processing in a simple, composable and maintainable way. A persistence storage
in MongoDB is used to ensure fault-tolerant pullers: there is no event loss or duplication even in case of failure of a puller. The system is easily distributable 
thanks location transparency that is inherent to the actor model.

The Stream Processing part uses a complex adaptive push-pull model with back-pressure to allow decoupled stream processing while optimizing resource consumption. A stream processor abstraction has been created on top of Iteratees, Futures and Promises. The tree of processors guarantees in-order sequential asynchronous stream processing with fault-tolerance: the temporary failure of a processor is guaranteed without message loss or duplication by allowing a processor to replay the stream from its parent.
\\


\section{Conclusion}

In this thesis has been presented a Reactive platform for Data Integration and Event Stream Processing. Events and Event-Sourcing are the core concepts of the platform to enable 
real-time propagation of data changes across the system, from external data sources to data aggregation dashboards. Data sources are pulled in a parallel and incremental way via their REST API to create a real-time flow of events that are inserted in an event-sourced Journal. Saving not only the current state of the data but also all the data changes
that led to this current state allows to be able to query and replay the history of the data, which can be useful for business purposes but also for technical purposes such as fault-tolerance or stream processing with subscribers that have heterogeneous processing speeds.
From the events stored in the Journal, a tree of stream processors can be defined to subscribe to the data changes and react in various ways. An example use case of a processor is maintaining a specific data aggregation dashboard that is updated incrementally upon certain types of events in real-time. Strong technical guarantees are ensured by the platform as in-order sequential asynchronous processing with exactly-once side-effect semantic even in case of failure. Under the hood, a complex adaptive push-pull model with back-pressure is used to maximize the performance of the system and minimize the amount of resource (CPU, RAM) used. Functional programming abstractions have been used for maintainable and composable asynchronous programming.
\\

Concerning the Future Work that can be made on the platform, one problem is that the distributed mode is for now point-to-point oriented, i.e. we need to manually configure which component (data puller, processor) is on which machine. To obtain elastic scalability, it would be interesting to use a cluster oriented approach with a cluster manager layer that automatically put components on machines according to the current resource availability of each machine.


