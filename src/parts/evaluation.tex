\chapter{Evaluation}

\section{Requirement evaluation}

Concerning the functional requirements, the platform meets the needs by allowing to define data pullers (actors) that incrementally pull various data sources and various resource types.
The pulled data is processed sequentially in a simple pipeline that aggregates, cleans and validates the data before inserting events in the Journal.

Then, the Stream Processing part allows to define a tree of stream processors. A processor can react to events sent by its parent by producing a substream of events towards its children. Substream are inserted in-place in the stream, meaning that the whole substream should be sent to children before processing the next input event. A processor can do a side-effect with a guaranteed exactly-once side-effect semantic, allowing to the user of the library to safely define non-idempotent side-effects.
\\

Concerning the non-functional requirements, all the parts of the architecture are built with an asynchronous non-blocking architecture according to the Reactive Manifesto 
\footfullcite{bib:reactiveManifesto} to optimize performance and resource use by being \textit{event-driven, scalable, resilient and responsive}, the four Reactive Traits.

The Data Integration part is built on top of an Actor system to allow easy concurrency and distribution. Moreover, it makes the best use of resources (CPU, Threads) thanks to a non-blocking implementation. Futures and Iteratees are used to model sequential in-order asynchronous stream processing in a simple, composable and maintainable way. A persistence storage
in MongoDB is used to ensure fault-tolerant pullers: there is no event loss or duplication even in case of failure of a puller. The system is easily distributable 
thanks location transparency that is inherent to the actor model.

The Stream Processing part uses a complex adaptive push-pull model with back-pressure to allow decoupled stream processing while optimizing resource consumption. A stream processor abstraction has been created on top of Iteratees, Futures and Promises. The tree of processors guarantees in-order sequential asynchronous stream processing with fault-tolerance: the temporary failure of a processor is guaranteed without message loss or duplication by allowing a processor to replay the stream from its parent (as shown and explained in section 
\ref{sec:archistream}).


\section{Performance evaluation}

\subsection{Latency and resource consumption}

Performance tests have been performed on the Journal and Stream Processing part on a local machine with a 2.2 GHz processor of 8 cores (so a parallelization factor of 8). We will perform tests on the business use case application described in section \ref{sec:usecasebusiness}.
\\

First, we measure the end-to-end latency between the time when an event is inserted and the time when the resulting dashboard update(s) have been entirely performed. Figure \ref{fig:latencyplot} show the plot of the end-to-end latency between the Journal and a Dashboard when we increase the push rate of events inserted in the Journal.

We notice that before a threshold of roughly 170 events inserted in the Journal per second, the latency between the Journal and a Dashboard is constant at 8 ms. This means that the processors lower in the tree structure (snapshot, flatSnapshot and the dashboards) can handle the push rate of the Journal and are not late in the stream (push-mode).
However, after 170 events per second, the latency becomes linear with the push rate. This means that dashboards starts to be late in the stream, and are forced to replay events at their rate because their processing time is too slow. Thus, the resultant plot is either constant (up-to-date child processors) or linear (late child processors), which is an expected and good result for scalability.
\\

\begin{figure}[h]
  \begin{center} 
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/plotlatency.png}}
    \caption{End-to-end latency between the Journal and a Dashboard while varying the Journal push rate}
    \label{fig:latencyplot}
  \end{center}
\end{figure}

Furthermore, during this performance test, the resource consumption (JVM Heap space, number of threads) has been profiled. Figure \ref{fig:plotheapspace} and Figure \ref{fig:plotthreads} show the JVM Heap space used and the number of threads used while increasing the event push rate on the Journal. We notice that resource consumption is constant even when we increase a lot the push event rate, which validates the fact that the platform uses a predictable amount of resource that does not increase too much with the load (optimization of resource consumption).

\begin{figure}[h]
  \begin{center} 
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/plotheapspace.png}}
    \caption{JVM heap space consumption while varying the Journal push rate}
    \label{fig:plotheapspace}
  \end{center}
\end{figure}

\begin{figure}[h]
  \begin{center} 
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/plotthreads.png}}
    \caption{Number of threads used while varying the Journal push rate}
    \label{fig:plotthreads}
  \end{center}
\end{figure}


\subsection{Fault-tolerance}

In this part, we define a performance test to measure the recovery time of a processor that recovers from a crash and must replay 1000 events that happened when it was crashed. Figure \ref{fig:barchart} shows the resultant bar chart.
Using the tree structure of the business use case application presented in section \ref{sec:usecasebusiness}, the Snapshot processor is first killed and then restart. Its parent is the Journal, a persistent processor, so it has only one level to climb in the tree to replay the stream. Then, FlatSnapshot is killed. It is at level 2 in the tree, but its parent is a persistent processor (Snapshot). Therefore, it has also only one level in the tree to climb to replay the stream. As a result, its replay time is roughly the same than Snapshot (the processing time of these two processors is equivalent). Last but not least, a Dashboard is killed. As its parent is a side-effect processor (FlatSnapshot), it has to climb 2 levels to replay the stream (until Snapshot). Moreover, the processing time of a dashboard is slightly superior than other processors. As a result of these two factors (side-effect parent and slightly longer processing time), we see that Dashboard takes more time to replay the 1000 events that it missed, which is expected according to our model.

\begin{figure}[h]
  \begin{center} 
    \makebox[\textwidth]{\includegraphics[width=1.0\textwidth]{img/barchart.png}}
    \caption{Recovery time of processors for a replay of 1000 events}
    \label{fig:barchart}
  \end{center}
\end{figure}


